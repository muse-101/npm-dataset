# -------------------------------------------------------------
# ğŸ’¡ åµŒå…¥ Hugo Page èªªæ˜
# å¯ç›´æ¥ç”¨ iframe åµŒå…¥ä¸åŒ CSVï¼Œä¾‹å¦‚ï¼š
# <iframe src="https://npm-dataset.streamlit.app/?embed=true&csv=d01éŠ…_s1.csv" width="100%" height="900" style="border:0;" loading="lazy"></iframe>
# è‹¥è¦ç°¡åŒ–ï¼Œå¯åœ¨ Hugo æ–°å¢ shortcodeï¼šlayouts/shortcodes/streamlit.html
# å…§å®¹ï¼š<iframe src="https://npm-dataset.streamlit.app/?embed=true&csv={{ .Get \"csv\" | urlquery }}" width="100%" height="900" style="border:0;" loading="lazy"></iframe>
# ä½¿ç”¨ï¼š{{< streamlit csv="d02ç‰_s1.csv" >}}
# -------------------------------------------------------------

# === è­·çœ¼ç°è—ä¸»é¡Œï¼ˆæŸ”å’Œç°åº•ï¼‹æ·ºç°è—ä¸»è‰²ï¼‰ ===
import streamlit as st
st.set_page_config(page_title="CSV å…¸è—è³‡æ–™ç€è¦½å™¨", layout="wide")
st.markdown(
    """
    <style>
    body, .stApp { background-color: #F4F5F7 !important; color: #111827 !important; }
    div[data-testid="stDataFrame"] div[role="gridcell"] {
        background-color: #FAFBFD !important; color: #111827 !important;
        padding-top: 0.75rem !important; padding-bottom: 0.75rem !important;
    }
    div[data-testid="stDataFrame"] div[role="columnheader"] {
        background-color: #EEF2F7 !important; color: #111827 !important;
        padding-top: 0.75rem !important; padding-bottom: 0.75rem !important;
    }
    .stTextInput > div > div > input { background-color: #FFFFFF !important; color: #111827 !important; }
    .stSelectbox div[data-baseweb="select"] { background-color: #FFFFFF !important; color: #111827 !important; }
    .stDownloadButton button, .stButton button { background-color: #5A7BD8 !important; color: #fff !important; border: none !important; }
    .stDownloadButton button:hover, .stButton button:hover { background-color: #7395EB !important; color: #fff !important; }
    .stExpander, .stSelectbox, .stTextInput, .stMultiSelect, .stDataFrame { border-radius: 10px !important; box-shadow: 0 1px 4px rgba(0,0,0,0.1); }
    /* è®“å…§å±¤ App åœ¨ iframe è£¡æ’æ»¿é«˜åº¦ï¼Œé¿å…å…§å®¹è¢«å£“ç¸® */
html, body, .stApp { min-height: 100% !important; height: auto !important; overflow: visible !important; }
#root, section.main, .main { min-height: 100% !important; height: auto !important; }
/* åµŒå…¥æ¨¡å¼å»æ‰å¤šé¤˜ä¸Šä¸‹å…§è·ï¼Œé¿å…æœ‰æ•ˆé«˜åº¦è¢«åƒæ‰ */
.block-container { min-height: calc(100% - 16px) !important; height: auto !important; padding-top: 1.25rem !important; padding-bottom: 1rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

# ===== ä¸»é«” =====
import os
import math
import duckdb
import pandas as pd
from datetime import datetime
from urllib.parse import urlparse, unquote, urlunparse, quote

st.title("CSV å…¸è—è³‡æ–™ç€è¦½å™¨")
src_ph = st.empty()

# === å›ºå®šè®€åŒå±¤ CSVï¼ˆé è¨­æœ¬åœ° d0.csvï¼›å¯è¢« ?csv= æˆ–å´æ¬„ URL è¦†è“‹ï¼‰ ===
CSV_NAME = "d0.csv"
IMAGE_COL_OVERRIDE = "imageUrl_s"
CSV_PATH = os.path.join(os.path.dirname(__file__), CSV_NAME)
DEFAULT_CSV_URL = ""  # ç©ºå­—ä¸² = é è¨­èµ°æœ¬åœ° d0.csv
# âœ… é ç«¯ CSV åŸºåº•ï¼ˆç”¨æ–¼ ?csv=åƒ…çµ¦æª”åæ™‚çš„å¾Œæ´è§£æï¼‰ï¼Œå¯ç”¨ç’°å¢ƒè®Šæ•¸ CSV_BASE_URL è¦†è“‹
CSV_BASE_URL = os.environ.get("CSV_BASE_URL", "https://raw.githubusercontent.com/muse-101/npm-dataset/main/")
REMOTE_CSV_BASES = [CSV_BASE_URL]

con = duckdb.connect()
con.execute("INSTALL httpfs; LOAD httpfs;")
# è¨­å®š User-Agentï¼Œé¿å…éƒ¨åˆ†é ç«¯ï¼ˆå« GitHub Raw/CDNï¼‰æ‹’çµ•ç©º UA
try:
    con.execute("SET http_user_agent='Mozilla/5.0 (Streamlit DuckDB)';")
except Exception:
    pass
# æ›´ç©©å®šçš„ httpfs è¨­å®šï¼ˆé ç«¯ CSV è®€å–å„ªåŒ–ï¼‰
for _sql in [
    "SET enable_http_metadata_cache=true;",
    "SET http_keep_alive=true;",
    "SET http_max_redirects=10;",
    "SET http_open_timeout=30;",
    "SET http_reuse_connections=true;",
]:
    try:
        con.execute(_sql)
    except Exception:
        pass

# â€”â€” å·¦å´æ¬„ï¼šæ§åˆ¶é¢æ¿ï¼ˆå¿«é€Ÿé€£çµ + URL è¼‰å…¥ + æ¬„ä½/æœå°‹/é é¢å¤§å° + ä¸‹è¼‰ï¼‰â€”â€”
import urllib.parse as _u
with st.sidebar:
    st.header("æ§åˆ¶é¢æ¿")

    # æœ¬æ©Ÿå¿«é€Ÿåˆ‡æ›ï¼šæ¸¬è©¦æª”é€£çµï¼ˆå¯è‡ªè¡Œå¢ä¿®ï¼‰
    TEST_FILES = [
        "d0.csv",
        "d01éŠ…_s1.csv", "d02ç‰_s1.csv", "d03ç“·_s1.csv", "d04çº_s1.csv", "d05é›œ_s1.csv",
        "d06æ–‡_s1.csv", "d07ç¹”_s1.csv", "d08é›•_s1.csv", "d09æ¼†_s1.csv", "d10éŒ¢_s1.csv",
        "d20ç•«_s1.csv", "d21æ›¸_s1.csv", "d22å¸–_s1.csv", "d23æ‰‡_s1.csv", "d24çµ²_s1.csv"
    ]
    def _display_name(fn: str) -> str: return fn[:-4] if fn.lower().endswith(".csv") else fn
    links = " | ".join([f"[{_display_name(n)}](?csv={_u.quote(n)})" for n in TEST_FILES])
    st.subheader("åˆ‡æ›æ¸¬è©¦æª”")
    st.markdown(links, unsafe_allow_html=True)
    st.markdown("---")

    # ç›´æ¥è²¼ URL è¼‰å…¥ï¼ˆGitHub Raw / Google Drive uc?export=download / ä»»æ„ http/httpsï¼‰
    st.subheader("è²¼ä¸Š URL è¼‰å…¥")
    csv_url = st.text_input(
        "é ç«¯è³‡æ–™ç¶²å€ï¼ˆhttp/httpsï¼‰",
        value="",
        placeholder="ä¾‹å¦‚ï¼šhttps://raw.githubusercontent.com/â€¦/d01éŠ…_s1.csv æˆ– https://â€¦/file.parquet",
        key="csv_url_input",
    )
    col_u1, col_u2 = st.columns([1,1])
    with col_u1:
        if st.button("è¼‰å…¥ URL", type="primary"):
            if csv_url.strip():
                url = csv_url.strip()
                try:
                    st.query_params.update({"csv": url})
                except Exception:
                    st.experimental_set_query_params(csv=url)
                st.rerun()
    with col_u2:
        if st.button("æ¸…é™¤ URL"):
            try:
                st.query_params.clear()
            except Exception:
                st.experimental_set_query_params()
            st.rerun()

# === è§£æç¶²å€åƒæ•¸ï¼ˆ?csv= å¯ç‚º æœ¬åœ°æª”å æˆ– http/https URLï¼‰ ===
RESOLVED_URL = None  # è‹¥å¯¦éš›ç”¨åˆ°é ç«¯ç¶²å€ï¼Œè¨˜éŒ„åœ¨æ­¤ä¾›å¾Œæ´è¼‰å…¥ä½¿ç”¨

def _normalize_drive_url(u: str) -> str:
    """å°‡å¸¸è¦‹çš„ Google Drive åˆ†äº«ç¶²å€è½‰ç‚ºå¯ç›´æ¥ä¸‹è¼‰çš„ uc?export=download å½¢å¼ã€‚"""
    try:
        if "drive.google.com" not in u and "docs.google.com" not in u:
            return u
        import re
        m = re.search(r"/file/d/([a-zA-Z0-9_-]+)", u)
        if m:
            return f"https://drive.google.com/uc?export=download&id={m.group(1)}"
        m = re.search(r"[?&]id=([a-zA-Z0-9_-]+)", u)
        if m:
            return f"https://drive.google.com/uc?export=download&id={m.group(1)}"
        if "/uc?" in u:
            return u
    except Exception:
        pass
    return u

def _auto_encode_nonascii_url(u: str) -> str:
    """è‹¥ URL path ä¸­å«æœ‰é ASCIIï¼ˆä¾‹å¦‚ä¸­æ–‡æª”åï¼‰ï¼Œåªç·¨ç¢¼ path éƒ¨åˆ†ï¼Œé¿å…å¾Œç«¯ä¸èªå¾—ã€‚
    ä¾‹å¦‚ï¼šhttps://raw.githubusercontent.com/.../d01éŠ…_s1.csv -> path æœƒè¢« quote æˆ %E9%8A%85
    """
    try:
        if not u or not isinstance(u, str) or not u.lower().startswith(("http://","https://")):
            return u
        pr = urlparse(u)
        # åª re-encode pathï¼›query / fragment ä¿ç•™ï¼ˆHugo ç«¯å·²è™•ç†ï¼‰
        new_path = quote(pr.path, safe="/.-_~")
        if new_path == pr.path:
            return u
        return urlunparse((pr.scheme, pr.netloc, new_path, pr.params, pr.query, pr.fragment))
    except Exception:
        return u

        pr = urlparse(u)
        # åª re-encode pathï¼›query / fragment ä¿ç•™ï¼ˆå·²ç”± Hugo urlquery è™•ç†ï¼‰
        new_path = quote(pr.path, safe="/.-_~")  # å¸¸ç”¨å®‰å…¨å­—ä¿ç•™
        if new_path == pr.path:
            return u
        return urlunparse((pr.scheme, pr.netloc, new_path, pr.params, pr.query, pr.fragment))
    except Exception:
        return u
        # å½¢å¼1ï¼š/file/d/<id>/view
        import re
        m = re.search(r"/file/d/([a-zA-Z0-9_-]+)", u)
        if m:
            return f"https://drive.google.com/uc?export=download&id={m.group(1)}"
        # å½¢å¼2ï¼šopen?id=<id>
        m = re.search(r"[?&]id=([a-zA-Z0-9_-]+)", u)
        if m:
            return f"https://drive.google.com/uc?export=download&id={m.group(1)}"
        # è‹¥æœ¬ä¾†å°±æ˜¯ uc? å½¢å¼ï¼Œç›´æ¥å›å‚³
        if "/uc?" in u:
            return u
    except Exception:
        pass
    return u
try:
    _qp = st.query_params if hasattr(st, "query_params") else st.experimental_get_query_params()
except Exception:
    _qp = {}
_csv_param = _qp.get("csv", "")
if isinstance(_csv_param, list):
    _csv_param = _csv_param[0] if _csv_param else ""

if _csv_param:
    if str(_csv_param).lower().startswith(("http://", "https://")):
        _url = _auto_encode_nonascii_url(_normalize_drive_url(str(_csv_param)))
        _is_parquet = _url.lower().endswith(".parquet")
        scan = f"parquet_scan('{_url}')" if _is_parquet else f"read_csv_auto('{_url}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼ˆURLï¼‰ï¼š{_url}"
        RESOLVED_URL = _url
    else:
        _alt = os.path.join(os.path.dirname(__file__), _csv_param)
        if os.path.exists(_alt):
            scan = f"read_csv_auto('{_alt}', SAMPLE_SIZE=200000)"
            source_hint = f"è³‡æ–™ä¾†æºï¼ˆåŒå±¤æª”æ¡ˆï¼‰ï¼š{_alt}"
        else:
            # å¾Œæ´ï¼šå°‡ csv æª”åæ¥åˆ°é ç«¯åŸºåº•ï¼ˆä¾‹å¦‚ GitHub Rawï¼‰ï¼Œæ”¯æ´ä¸­æ–‡æª”å
            enc = _u.quote(_csv_param)
            fallback_url = REMOTE_CSV_BASES[0].rstrip('/') + '/' + enc
            scan = f"read_csv_auto('{fallback_url}', SAMPLE_SIZE=200000)"
            source_hint = f"è³‡æ–™ä¾†æºï¼ˆé ç«¯å¾Œæ´ï¼‰ï¼š{fallback_url}"
else:
    if DEFAULT_CSV_URL:
        _url = _auto_encode_nonascii_url(_normalize_drive_url(DEFAULT_CSV_URL))
        scan = f"read_csv_auto('{_url}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼ˆGitHub Rawï¼‰ï¼š{_url}"
        RESOLVED_URL = _url
    else:
        scan = f"read_csv_auto('{CSV_PATH}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼š{CSV_PATH}"

# === ä¸‹è¼‰æª”åï¼šmuz01_XXXX_OOOO.csv ===
# XXXX = åŸå§‹è³‡æ–™æª”åï¼ˆç„¡å‰¯æª”åï¼‰ï¼ŒOOOO = ç›®å‰æ™‚é–“æˆ³ï¼ˆYYYYMMDD_HHMMSSï¼‰

def _infer_src_basename(src: str) -> str:
    try:
        if str(src).lower().startswith(("http://","https://")):
            last = os.path.basename(urlparse(src).path)
            return os.path.splitext(unquote(last))[0] or "data"
        else:
            return os.path.splitext(os.path.basename(src))[0]
    except Exception:
        return "data"

_src_for_name = _csv_param or (DEFAULT_CSV_URL if DEFAULT_CSV_URL else CSV_PATH)
SRC_BASENAME = _infer_src_basename(_src_for_name)
TS = datetime.now().strftime("%Y%m%d_%H%M%S")
DL_NAME = f"muz01_{SRC_BASENAME}_{TS}.csv"
DL_STEM = DL_NAME[:-4] if DL_NAME.lower().endswith('.csv') else DL_NAME

src_ph.caption(source_hint)
st.success("ç›®å‰é è¨­è¼‰å…¥æœ¬åœ°æª”æ¡ˆ d0.csvï¼Œå¯ç”¨ ?csv= æˆ–å´æ¬„è²¼ä¸Š URL è®Šæ›´ä¾†æºã€‚")

# === åµéŒ¯å€ï¼ˆ?debug=1 æ™‚é¡¯ç¤ºè§£æå¾Œåƒæ•¸ï¼‰ ===
try:
    _dbg = _qp.get("debug", "")
    _dbg = _dbg[0] if isinstance(_dbg, list) else _dbg
    if str(_dbg) in ("1","true","yes"):
        st.info({
            "csv_param_raw": _csv_param,
            "resolved_url": RESOLVED_URL,
            "source_hint": source_hint,
        })
except Exception:
    pass

# === é è¦½æ¬„ä½ï¼ˆä»¥ä¾¿ç”Ÿæˆ UIï¼‰ ===
try:
    preview = con.execute(f"SELECT * FROM {scan} LIMIT 1").fetchdf()
except Exception as e:
    # å¾Œæ´ï¼šå°é ç«¯ URL å˜—è©¦ç”¨ pandas è¼‰å…¥ï¼Œå†è¨»å†Šæˆ DuckDB è‡¨æ™‚ view
    if RESOLVED_URL:
        try:
            if RESOLVED_URL.lower().endswith('.parquet'):
                _df_all = pd.read_parquet(RESOLVED_URL)
            else:
                _df_all = pd.read_csv(RESOLVED_URL)
            con.register('remote_fallback', _df_all)
            scan = 'remote_fallback'
            source_hint = source_hint + "ï¼ˆpandas å¾Œæ´è¼‰å…¥ï¼‰"
            preview = con.execute(f"SELECT * FROM {scan} LIMIT 1").fetchdf()
        except Exception as ee:
            st.error(f"""è®€å–è³‡æ–™çµæ§‹å¤±æ•—ï¼š{e}
é ç«¯å¾Œæ´ä¹Ÿå¤±æ•—ï¼š{ee}
è«‹ç¢ºèªé€£çµæ˜¯å¦ç‚ºå¯ç›´æ¥ä¸‹è¼‰çš„åŸå§‹æª”ï¼ˆGitHub Raw / Drive uc?export=downloadï¼‰ã€‚""")
            st.stop()
    else:
        st.error(f"è®€å–è³‡æ–™çµæ§‹å¤±æ•—ï¼š{e}")
        st.stop()

cols = preview.columns.tolist()
if not cols:
    st.error("CSV æ²’æœ‰æ¬„ä½ã€‚")
    st.stop()

# ğŸ” é©—è­‰å¿…å‚™æ¬„ä½ï¼šsk1ã€sk2ã€sk3ï¼ˆè¡¨æ ¼ä»å¯ç€è¦½ï¼›Sankey/skç¯€é»ç¼ºå‰‡æç¤ºï¼‰
REQUIRED_SK = ["sk1", "sk2", "sk3"]
missing_sk = [c for c in REQUIRED_SK if c not in cols]
if missing_sk:
    st.warning("""æœ¬ CSV ç¼ºå°‘å¿…å‚™æ¬„ä½ï¼š""" + ", ".join(missing_sk) + """ã€‚
è¡¨æ ¼ä»å¯ç€è¦½ï¼Œä½†ã€Œskç¯€é»ã€èˆ‡ã€ŒSankeyã€å°‡ç„¡æ³•æ­£ç¢ºé¡¯ç¤ºï¼›è«‹è£œä¸Š sk1ã€sk2ã€sk3 ä¸‰æ¬„ã€‚""")

# === å´æ¬„ï¼šæ¬„ä½èˆ‡æœå°‹ / æ¯é ç­†æ•¸ ===
with st.sidebar:
    st.subheader("æ¬„ä½èˆ‡æœå°‹")
    show_cols = st.multiselect("é¡¯ç¤ºæ¬„ä½", cols, default=cols[: min(10, len(cols))])
    kw_cols   = st.multiselect("é—œéµå­—æœå°‹æ¬„ä½", cols, default=show_cols or cols)
    page_size = st.selectbox("æ¯é ç­†æ•¸", [25, 50, 100, 200, 500], index=2)
    st.markdown("---")

# === æœå°‹æ¢ä»¶ ===
kw_value = st.session_state.get("keyword", "")
where = "TRUE"
params = {}
if kw_value and kw_cols:
    like_parts = [f'CAST("{c}" AS TEXT) ILIKE $kw' for c in kw_cols]
    where = "(" + " OR ".join(like_parts) + ")"
    params["kw"] = f"%{kw_value}%"

# === è¨ˆæ•¸ï¼ˆæœå°‹å¾Œ / åŸå§‹åŸºæº–ï¼‰ ===
try:
    total = con.execute(f"SELECT COUNT(*) FROM {scan} WHERE {where}", params).fetchone()[0]
    base_total = con.execute(f"SELECT COUNT(*) FROM {scan}").fetchone()[0]
except Exception as e:
    st.error(f"è¨ˆæ•¸å¤±æ•—ï¼š{e}")
    st.stop()

total_pages = max(1, math.ceil(total / page_size))
base_pages  = max(1, math.ceil(base_total / page_size))

# === åˆ†é æ§åˆ¶ ===
if "page" not in st.session_state or st.session_state.page < 1 or st.session_state.page > total_pages:
    st.session_state.page = 1
b1, b2, b3, b4 = st.columns(4)
with b1:
    if st.button("â® ç¬¬ä¸€é "): st.session_state.page = 1
with b2:
    if st.button("â—€ ä¸Šä¸€é ") and st.session_state.page > 1: st.session_state.page -= 1
with b3:
    if st.button("ä¸‹ä¸€é  â–¶") and st.session_state.page < total_pages: st.session_state.page += 1
with b4:
    if st.button("æœ€å¾Œä¸€é  â­"): st.session_state.page = total_pages
page = st.session_state.page

# === æŸ¥è©¢ç•¶é  ===
offset = (page - 1) * page_size
select_cols_sql = ", ".join([f'"{c}"' for c in (show_cols or cols)])
q_page = f"""
    SELECT {select_cols_sql}
    FROM {scan}
    WHERE {where}
    LIMIT {int(page_size)} OFFSET {int(offset)}
"""
try:
    df_page = con.execute(q_page, params).fetchdf()
except Exception as e:
    st.error(f"è®€å–é é¢è³‡æ–™å¤±æ•—ï¼š{e}")
    st.stop()

# === è‡ªå‹•è¾¨è­˜é€£çµæ¬„ï¼åœ–ç‰‡æ¬„ï¼ˆè¡¨æ ¼ç”¨ï¼‰ ===
def find_col(df: pd.DataFrame, candidates):
    cands = {c.lower() for c in candidates}
    for c in df.columns:
        if c.lower() in cands:
            return c
    return None

link_col  = find_col(df_page, {"url", "link", "api_link", "href"})
image_override = IMAGE_COL_OVERRIDE if (IMAGE_COL_OVERRIDE and IMAGE_COL_OVERRIDE in df_page.columns) else None
image_col = image_override or find_col(df_page, {"imageurl","image_url","imageurl_s","thumb","thumbnail","img","image"})
col_cfg = {}
if link_col:
    col_cfg[link_col] = st.column_config.LinkColumn(label=link_col, display_text="é–‹å•Ÿé€£çµ")
if image_col:
    col_cfg[image_col] = st.column_config.ImageColumn(label=image_col, help="ç¸®åœ–é è¦½")

# ================= Tabsï¼šè¡¨æ ¼ / skç¯€é» / Sankey =================
tab_table, tab_nodes, tab_sankey = st.tabs(["ğŸ“Š è¡¨æ ¼", "ğŸ”– skç¯€é»", "ğŸª¢ Sankey"]) 

with tab_table:
    st.subheader("è³‡æ–™è¡¨ï¼ˆç•¶é ï¼‰")
    # æŠŠæœå°‹è¼¸å…¥èˆ‡çµ±è¨ˆç§»åˆ°è¡¨æ ¼åˆ†é 
    keyword = st.text_input("é—œéµå­—ï¼ˆILIKE æ¨¡ç³Šæœå°‹ï¼‰", value=kw_value, key="keyword")
    if kw_value and kw_cols:
        st.write(f"ç¬¦åˆæ¢ä»¶ï¼š{total:,} ç­†ï¼›ç¬¬ {page} / {total_pages} é   ({base_total:,} ç­†ï¼›ç¬¬ 1 / {base_pages} é )")
    else:
        st.write(f"ç¬¦åˆæ¢ä»¶ï¼š{total:,} ç­†ï¼›ç¬¬ {page} / {total_pages} é ")
    st.data_editor(df_page, column_config=col_cfg, use_container_width=True, hide_index=True, disabled=True)

with tab_nodes:
    st.subheader("sk ç¯€é»ï¼ˆä¸å¥—ç”¨æœå°‹ / ä¸åˆ†é ï¼Œå›ºå®šé¡¯ç¤º id, sk1, sk2, sk3ï¼‰")
    if missing_sk:
        st.error("æ­¤ CSV ä¸åŒ…å« sk1ã€sk2ã€sk3 ä¸‰æ¬„ï¼Œç„¡æ³•é¡¯ç¤º sk ç¯€é»è¡¨ã€‚è«‹è£œé½Šå¾Œå†è©¦ã€‚")
    else:
        # è®€å–å®Œæ•´è³‡æ–™ï¼ˆå¿½ç•¥ WHERE èˆ‡åˆ†é ï¼‰ï¼Œåƒ…å–éœ€è¦çš„æ¬„ä½
        base_cols = ["sk1", "sk2", "sk3"]
        cols_exist = [c for c in ["id"] + base_cols if c in cols]
        sel_cols_nodes = ", ".join([f'"{c}"' for c in cols_exist])
        q_nodes = f"SELECT {sel_cols_nodes} FROM {scan}"
        df_nodes_full = con.execute(q_nodes).fetchdf().fillna("ï¼ˆç¼ºå€¼ï¼‰")
        if "id" not in df_nodes_full.columns:
            df_nodes_full.insert(0, "id", "")
        df_nodes_full = df_nodes_full[["id", "sk1", "sk2", "sk3"]]

        view = st.radio("æª¢è¦–æ–¹å¼", ["åŸå§‹åˆ—ï¼ˆid, sk1, sk2, sk3ï¼‰", "å”¯ä¸€çµ„åˆ + è¨ˆæ•¸ï¼ˆsk1, sk2, sk3ï¼‰"], horizontal=True)
        if view.startswith("å”¯ä¸€"):
            df_nodes = (
                df_nodes_full[["sk1","sk2","sk3"]]
                .value_counts(["sk1","sk2","sk3"])  # pandas >= 1.4
                .rename("count").reset_index()
                .sort_values("count", ascending=False)
            )
        else:
            df_nodes = df_nodes_full

        st.data_editor(df_nodes, use_container_width=True, hide_index=True, disabled=True)

        cna1, cna2 = st.columns(2)
        with cna1:
            st.download_button("ä¸‹è¼‰ sk ç¯€é»ï¼ˆç•¶å‰æª¢è¦–ï¼‰", data=df_nodes.to_csv(index=False).encode("utf-8-sig"), file_name=DL_STEM + "_nodes.csv", mime="text/csv")
        with cna2:
            st.download_button("ä¸‹è¼‰ sk åŸå§‹ï¼ˆid, sk1, sk2, sk3ï¼‰", data=df_nodes_full.to_csv(index=False).encode("utf-8-sig"), file_name=DL_STEM + "_raw.csv", mime="text/csv")

with tab_sankey:
    st.subheader("Sankeyï¼ˆå›ºå®šä½¿ç”¨ sk1 â†’ sk2 â†’ sk3ï¼›ä¸å¥—ç”¨æœå°‹ã€ä¸åˆ†é ï¼‰")
    if missing_sk:
        st.error("æ­¤ CSV ä¸åŒ…å« sk1ã€sk2ã€sk3 ä¸‰æ¬„ï¼Œç„¡æ³•ç¹ªè£½ Sankeyã€‚è«‹è£œé½Šå¾Œå†è©¦ã€‚")
    else:
        try:
            import plotly.graph_objects as go
        except ModuleNotFoundError:
            st.error("""æ‰¾ä¸åˆ° Plotlyã€‚è«‹å…ˆå®‰è£ï¼š`pip install plotly` æˆ– `pip3 install plotly`ã€‚è‹¥ç”¨ Condaï¼š`conda install -c plotly plotly`ã€‚
ï¼ˆTabs å·²é¡¯ç¤ºï¼›å®‰è£å¾Œé‡å•Ÿå³å¯é¡¯ç¤º Sankeyï¼‰""")
        else:
            # å–èˆ‡ã€Œskç¯€é»ã€ä¸€è‡´çš„è³‡æ–™ä¾†æºï¼ˆå®Œæ•´ã€ä¸åˆ†é ã€ä¸æœå°‹ï¼‰
            q_nodes_for_sankey = f"SELECT \"sk1\", \"sk2\", \"sk3\" FROM {scan}"
            df_nodes_for_sankey = con.execute(q_nodes_for_sankey).fetchdf().fillna("ï¼ˆç¼ºå€¼ï¼‰")
            work = df_nodes_for_sankey[["sk1","sk2","sk3"]].copy()

            # å»ºç«‹ linksï¼ˆsk1â†’sk2ã€sk2â†’sk3ï¼‰
            links = []
            for a, b in [("sk1","sk2"),("sk2","sk3")]:
                g = work.groupby([a,b], dropna=False, as_index=False).size()
                g.rename(columns={"size":"value", a:"src", b:"dst"}, inplace=True)
                links.append(g)
            links_df = pd.concat(links, ignore_index=True)

            vmax = int(max(1, int(links_df["value"].max())))
            min_val = st.slider("éæ¿¾ï¼šæœ€å°æ¬Šé‡", 1, vmax, value=1)
            links_df = links_df[links_df["value"] >= min_val]

            if links_df.empty:
                st.info("éæ¿¾å¾Œæ²’æœ‰é€£ç·šå¯é¡¯ç¤ºï¼Œè«‹æ”¾å¯¬é–€æª»æˆ–æ›´æ›è³‡æ–™æ¢ä»¶ã€‚")
            else:
                labels = pd.unique(pd.concat([work["sk1"], work["sk2"], work["sk3"]], ignore_index=True)).tolist()
                idx = {lab: i for i, lab in enumerate(labels)}
                links_df = links_df.assign(
                    source_id=links_df["src"].map(idx),
                    target_id=links_df["dst"].map(idx),
                )

                fig = go.Figure(data=[go.Sankey(
                    node=dict(
                        label=labels,
                        pad=20,
                        thickness=18,
                        color="#FFFFFF",
                        line=dict(color="rgba(0,0,0,0)", width=0),
                    ),
                    link=dict(
                        source=links_df["source_id"],
                        target=links_df["target_id"],
                        value=links_df["value"],
                        color="rgba(90,123,216,0.18)",
                    ),
                    textfont=dict(color="#0B1220", size=16, family="Microsoft JhengHei, Heiti TC, sans-serif"),
                )])
                fig.update_layout(
                    title_text="Sankeyï¼šsk1 â†’ sk2 â†’ sk3",
                    font_size=16,
                    font=dict(family="Microsoft JhengHei, Heiti TC, sans-serif", color="#0B1220"),
                    paper_bgcolor="#FFFFFF",
                    plot_bgcolor="#FFFFFF",
                    margin=dict(l=10, r=10, t=40, b=10),
                    hoverlabel=dict(bgcolor="#FFFFFF", font_size=14, font_family="Microsoft JhengHei, Heiti TC, sans-serif"),
                )
                st.plotly_chart(fig, use_container_width=True)

# â€”â€” å´æ¬„ï¼šä¸‹è¼‰å€ â€”â€”
with st.sidebar:
    st.subheader("ä¸‹è¼‰")
    st.caption("ä¸‹è¼‰ç•¶å‰é é¢æˆ–å®Œæ•´ç¯©é¸çµæœ")
    st.download_button("ä¸‹è¼‰ç•¶é ", df_page.to_csv(index=False).encode("utf-8"), DL_NAME, "text/csv")
    if st.button("ç”¢ç”Ÿå®Œæ•´ CSV"):
        out = "/tmp/filtered.csv"
        con.execute(
            f"COPY (SELECT {select_cols_sql} FROM {scan} WHERE {where}) TO '{out}' (HEADER, DELIMITER ',')",
            params,
        )
        with open(out, "rb") as f:
            st.download_button("ä¸‹è¼‰å®Œæ•´çµæœ", f, DL_NAME, "text/csv")

# ===ï¼ˆé€²éšï¼‰Hugo è‡ªé©æ‡‰é«˜åº¦ï¼šå¾ App å›å‚³å…§å®¹é«˜åº¦çµ¦çˆ¶é  ===
# èªªæ˜ï¼šFirefox åœ¨ iframe å…§å° vh è¨ˆç®—è¼ƒåš´æ ¼ï¼Œå»ºè­°æ”¹æˆ parent <-> iframe çš„ postMessage é€šè¨Š
# ç”¨æ³•ï¼š
#  1) æœ¬æ®µæœƒå®šæœŸæŠŠ document é«˜åº¦å›å‚³çµ¦ parentï¼›
#  2) è«‹åœ¨ Hugo shortcode åŠ ä¸Š window.addEventListener('message', ...) æ¥æ”¶ä¸¦è¨­å®šå°æ‡‰ iframe çš„ style.heightã€‚
try:
    from streamlit.components.v1 import html as _html
    _is_embed = str((_qp.get("embed") if not isinstance(_qp.get("embed"), list) else _qp.get("embed")[0]) or "").lower() == "true"
    if _is_embed:
        _html(
            """
            <script>
            (function(){
              function sendSize(){
                var h = Math.max(document.documentElement.scrollHeight, document.body.scrollHeight);
                try{ parent.postMessage({ kind: 'sl-size', height: h }, '*'); }catch(e){}
              }
              var ro = new ResizeObserver(function(){ sendSize(); });
              ro.observe(document.documentElement);
              window.addEventListener('load', sendSize);
              setInterval(sendSize, 800);
              // åˆæ¬¡è§¸ç™¼
              sendSize();
            })();
            </script>
            """,
            height=0,
            width=0,
        )
except Exception:
    pass
