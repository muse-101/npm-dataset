# -------------------------------------------------------------
# ğŸ’¡ åµŒå…¥ Hugo Page èªªæ˜
# å¯ç›´æ¥ç”¨ iframe åµŒå…¥ä¸åŒ CSVï¼Œä¾‹å¦‚ï¼š
# <iframe src="https://npm-dataset.streamlit.app/?embed=true&csv=d01éŠ…_s1.csv" width="100%" height="900" style="border:0;" loading="lazy"></iframe>
# è‹¥è¦ç°¡åŒ–ï¼Œå¯åœ¨ Hugo æ–°å¢ shortcodeï¼šlayouts/shortcodes/streamlit.html
# å…§å®¹ï¼š<iframe src="https://npm-dataset.streamlit.app/?embed=true&csv={{ .Get \"csv\" | urlquery }}" width="100%" height="900" style="border:0;" loading="lazy"></iframe>
# ä½¿ç”¨ï¼š{{< streamlit csv="d02ç‰_s1.csv" >}}
# -------------------------------------------------------------

# === è­·çœ¼ç°è—ä¸»é¡Œï¼ˆæŸ”å’Œç°åº•ï¼‹æ·ºç°è—ä¸»è‰²ï¼‰ ===
import streamlit as st
st.set_page_config(page_title="CSV å…¸è—è³‡æ–™ç€è¦½å™¨", layout="wide")
st.markdown(
    """
    <style>
    body, .stApp { background-color: #F4F5F7 !important; color: #111827 !important; }
    div[data-testid="stDataFrame"] div[role="gridcell"] {
        background-color: #FAFBFD !important; color: #111827 !important;
        padding-top: 0.75rem !important; padding-bottom: 0.75rem !important;
    }
    div[data-testid="stDataFrame"] div[role="columnheader"] {
        background-color: #EEF2F7 !important; color: #111827 !important;
        padding-top: 0.75rem !important; padding-bottom: 0.75rem !important;
    }
    .stTextInput > div > div > input { background-color: #FFFFFF !important; color: #111827 !important; }
    .stSelectbox div[data-baseweb="select"] { background-color: #FFFFFF !important; color: #111827 !important; }
    .stDownloadButton button, .stButton button { background-color: #5A7BD8 !important; color: #fff !important; border: none !important; }
    .stDownloadButton button:hover, .stButton button:hover { background-color: #7395EB !important; color: #fff !important; }
    .stExpander, .stSelectbox, .stTextInput, .stMultiSelect, .stDataFrame { border-radius: 10px !important; box-shadow: 0 1px 4px rgba(0,0,0,0.1); }
    </style>
    """,
    unsafe_allow_html=True,
)

# ===== ä¸»é«” =====
import os
import math
import duckdb
import pandas as pd

st.title("CSV å…¸è—è³‡æ–™ç€è¦½å™¨")
src_ph = st.empty()

# === å›ºå®šè®€åŒå±¤ CSVï¼ˆé è¨­æœ¬åœ° d0.csvï¼›å¯è¢« ?csv= æˆ–å´æ¬„ URL è¦†è“‹ï¼‰ ===
CSV_NAME = "d0.csv"
IMAGE_COL_OVERRIDE = "imageUrl_s"
CSV_PATH = os.path.join(os.path.dirname(__file__), CSV_NAME)
DEFAULT_CSV_URL = ""  # ç©ºå­—ä¸² = é è¨­èµ°æœ¬åœ° d0.csv

con = duckdb.connect()
con.execute("INSTALL httpfs; LOAD httpfs;")

# â€”â€” å·¦å´æ¬„ï¼šæ§åˆ¶é¢æ¿ï¼ˆå¿«é€Ÿé€£çµ + URL è¼‰å…¥ + æ¬„ä½/æœå°‹/é é¢å¤§å° + ä¸‹è¼‰ï¼‰â€”â€”
import urllib.parse as _u
with st.sidebar:
    st.header("æ§åˆ¶é¢æ¿")

    # æœ¬æ©Ÿå¿«é€Ÿåˆ‡æ›ï¼šæ¸¬è©¦æª”é€£çµï¼ˆå¯è‡ªè¡Œå¢ä¿®ï¼‰
    TEST_FILES = [
        "d0.csv",
        "d01éŠ…_s1.csv", "d02ç‰_s1.csv", "d03ç“·_s1.csv", "d04çº_s1.csv", "d05é›œ_s1.csv",
        "d06æ–‡_s1.csv", "d07ç¹”_s1.csv", "d08é›•_s1.csv", "d09æ¼†_s1.csv", "d10éŒ¢_s1.csv",
        "d20ç•«_s1.csv", "d21æ›¸_s1.csv", "d22å¸–_s1.csv", "d23æ‰‡_s1.csv", "d24çµ²_s1.csv"
    ]
    def _display_name(fn: str) -> str: return fn[:-4] if fn.lower().endswith(".csv") else fn
    links = " | ".join([f"[{_display_name(n)}](?csv={_u.quote(n)})" for n in TEST_FILES])
    st.subheader("åˆ‡æ›æ¸¬è©¦æª”")
    st.markdown(links, unsafe_allow_html=True)
    st.markdown("---")

    # ç›´æ¥è²¼ URL è¼‰å…¥ï¼ˆGitHub Raw / Google Drive uc?export=download / ä»»æ„ http/httpsï¼‰
    st.subheader("è²¼ä¸Š URL è¼‰å…¥")
    csv_url = st.text_input(
        "é ç«¯è³‡æ–™ç¶²å€ï¼ˆhttp/httpsï¼‰",
        value="",
        placeholder="ä¾‹å¦‚ï¼šhttps://raw.githubusercontent.com/â€¦/d01éŠ…_s1.csv æˆ– https://â€¦/file.parquet",
        key="csv_url_input",
    )
    col_u1, col_u2 = st.columns([1,1])
    with col_u1:
        if st.button("è¼‰å…¥ URL", type="primary"):
            if csv_url.strip():
                url = csv_url.strip()
                try:
                    st.query_params.update({"csv": url})
                except Exception:
                    st.experimental_set_query_params(csv=url)
                st.rerun()
    with col_u2:
        if st.button("æ¸…é™¤ URL"):
            try:
                st.query_params.clear()
            except Exception:
                st.experimental_set_query_params()
            st.rerun()

# === è§£æç¶²å€åƒæ•¸ï¼ˆ?csv= å¯ç‚º æœ¬åœ°æª”å æˆ– http/https URLï¼‰ ===
try:
    _qp = st.query_params if hasattr(st, "query_params") else st.experimental_get_query_params()
except Exception:
    _qp = {}
_csv_param = _qp.get("csv", "")
if isinstance(_csv_param, list):
    _csv_param = _csv_param[0] if _csv_param else ""

if _csv_param:
    if str(_csv_param).lower().startswith(("http://", "https://")):
        _is_parquet = str(_csv_param).lower().endswith(".parquet")
        scan = f"parquet_scan('{_csv_param}')" if _is_parquet else f"read_csv_auto('{_csv_param}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼ˆURLï¼‰ï¼š{_csv_param}"
    else:
        _alt = os.path.join(os.path.dirname(__file__), _csv_param)
        if not os.path.exists(_alt):
            st.error(f"æ‰¾ä¸åˆ°æŒ‡å®šçš„ csv æª”ï¼š{_alt}")
            st.stop()
        scan = f"read_csv_auto('{_alt}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼ˆåŒå±¤æª”æ¡ˆï¼‰ï¼š{_alt}"
else:
    if DEFAULT_CSV_URL:
        scan = f"read_csv_auto('{DEFAULT_CSV_URL}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼ˆGitHub Rawï¼‰ï¼š{DEFAULT_CSV_URL}"
    else:
        scan = f"read_csv_auto('{CSV_PATH}', SAMPLE_SIZE=200000)"
        source_hint = f"è³‡æ–™ä¾†æºï¼š{CSV_PATH}"

src_ph.caption(source_hint)
st.success("ç›®å‰é è¨­è¼‰å…¥æœ¬åœ°æª”æ¡ˆ d0.csvï¼Œå¯ç”¨ ?csv= æˆ–å´æ¬„è²¼ä¸Š URL è®Šæ›´ä¾†æºã€‚")

# === é è¦½æ¬„ä½ï¼ˆä»¥ä¾¿ç”Ÿæˆ UIï¼‰ ===
try:
    preview = con.execute(f"SELECT * FROM {scan} LIMIT 1").fetchdf()
except Exception as e:
    st.error(f"è®€å–è³‡æ–™çµæ§‹å¤±æ•—ï¼š{e}")
    st.stop()

cols = preview.columns.tolist()
if not cols:
    st.error("CSV æ²’æœ‰æ¬„ä½ã€‚")
    st.stop()

# ğŸ” é©—è­‰å¿…å‚™æ¬„ä½ï¼šsk1ã€sk2ã€sk3ï¼ˆè¡¨æ ¼ä»å¯ç€è¦½ï¼›Sankey/skç¯€é»ç¼ºå‰‡æç¤ºï¼‰
REQUIRED_SK = ["sk1", "sk2", "sk3"]
missing_sk = [c for c in REQUIRED_SK if c not in cols]
if missing_sk:
    st.warning("""æœ¬ CSV ç¼ºå°‘å¿…å‚™æ¬„ä½ï¼š""" + ", ".join(missing_sk) + """ã€‚
è¡¨æ ¼ä»å¯ç€è¦½ï¼Œä½†ã€Œskç¯€é»ã€èˆ‡ã€ŒSankeyã€å°‡ç„¡æ³•æ­£ç¢ºé¡¯ç¤ºï¼›è«‹è£œä¸Š sk1ã€sk2ã€sk3 ä¸‰æ¬„ã€‚""")

# === å´æ¬„ï¼šæ¬„ä½èˆ‡æœå°‹ / æ¯é ç­†æ•¸ ===
with st.sidebar:
    st.subheader("æ¬„ä½èˆ‡æœå°‹")
    show_cols = st.multiselect("é¡¯ç¤ºæ¬„ä½", cols, default=cols[: min(10, len(cols))])
    kw_cols   = st.multiselect("é—œéµå­—æœå°‹æ¬„ä½", cols, default=show_cols or cols)
    page_size = st.selectbox("æ¯é ç­†æ•¸", [25, 50, 100, 200, 500], index=2)
    st.markdown("---")

# === æœå°‹æ¢ä»¶ ===
kw_value = st.session_state.get("keyword", "")
where = "TRUE"
params = {}
if kw_value and kw_cols:
    like_parts = [f'CAST("{c}" AS TEXT) ILIKE $kw' for c in kw_cols]
    where = "(" + " OR ".join(like_parts) + ")"
    params["kw"] = f"%{kw_value}%"

# === è¨ˆæ•¸ï¼ˆæœå°‹å¾Œ / åŸå§‹åŸºæº–ï¼‰ ===
try:
    total = con.execute(f"SELECT COUNT(*) FROM {scan} WHERE {where}", params).fetchone()[0]
    base_total = con.execute(f"SELECT COUNT(*) FROM {scan}").fetchone()[0]
except Exception as e:
    st.error(f"è¨ˆæ•¸å¤±æ•—ï¼š{e}")
    st.stop()

total_pages = max(1, math.ceil(total / page_size))
base_pages  = max(1, math.ceil(base_total / page_size))

# === åˆ†é æ§åˆ¶ ===
if "page" not in st.session_state or st.session_state.page < 1 or st.session_state.page > total_pages:
    st.session_state.page = 1
b1, b2, b3, b4 = st.columns(4)
with b1:
    if st.button("â® ç¬¬ä¸€é "): st.session_state.page = 1
with b2:
    if st.button("â—€ ä¸Šä¸€é ") and st.session_state.page > 1: st.session_state.page -= 1
with b3:
    if st.button("ä¸‹ä¸€é  â–¶") and st.session_state.page < total_pages: st.session_state.page += 1
with b4:
    if st.button("æœ€å¾Œä¸€é  â­"): st.session_state.page = total_pages
page = st.session_state.page

# === æŸ¥è©¢ç•¶é  ===
offset = (page - 1) * page_size
select_cols_sql = ", ".join([f'"{c}"' for c in (show_cols or cols)])
q_page = f"""
    SELECT {select_cols_sql}
    FROM {scan}
    WHERE {where}
    LIMIT {int(page_size)} OFFSET {int(offset)}
"""
try:
    df_page = con.execute(q_page, params).fetchdf()
except Exception as e:
    st.error(f"è®€å–é é¢è³‡æ–™å¤±æ•—ï¼š{e}")
    st.stop()

# === è‡ªå‹•è¾¨è­˜é€£çµæ¬„ï¼åœ–ç‰‡æ¬„ï¼ˆè¡¨æ ¼ç”¨ï¼‰ ===
def find_col(df: pd.DataFrame, candidates):
    cands = {c.lower() for c in candidates}
    for c in df.columns:
        if c.lower() in cands:
            return c
    return None

link_col  = find_col(df_page, {"url", "link", "api_link", "href"})
image_override = IMAGE_COL_OVERRIDE if (IMAGE_COL_OVERRIDE and IMAGE_COL_OVERRIDE in df_page.columns) else None
image_col = image_override or find_col(df_page, {"imageurl","image_url","imageurl_s","thumb","thumbnail","img","image"})
col_cfg = {}
if link_col:
    col_cfg[link_col] = st.column_config.LinkColumn(label=link_col, display_text="é–‹å•Ÿé€£çµ")
if image_col:
    col_cfg[image_col] = st.column_config.ImageColumn(label=image_col, help="ç¸®åœ–é è¦½")

# ================= Tabsï¼šè¡¨æ ¼ / skç¯€é» / Sankey =================
tab_table, tab_nodes, tab_sankey = st.tabs(["ğŸ“Š è¡¨æ ¼", "ğŸ”– skç¯€é»", "ğŸª¢ Sankey"]) 

with tab_table:
    st.subheader("è³‡æ–™è¡¨ï¼ˆç•¶é ï¼‰")
    # æŠŠæœå°‹è¼¸å…¥èˆ‡çµ±è¨ˆç§»åˆ°è¡¨æ ¼åˆ†é 
    keyword = st.text_input("é—œéµå­—ï¼ˆILIKE æ¨¡ç³Šæœå°‹ï¼‰", value=kw_value, key="keyword")
    if kw_value and kw_cols:
        st.write(f"ç¬¦åˆæ¢ä»¶ï¼š{total:,} ç­†ï¼›ç¬¬ {page} / {total_pages} é   ({base_total:,} ç­†ï¼›ç¬¬ 1 / {base_pages} é )")
    else:
        st.write(f"ç¬¦åˆæ¢ä»¶ï¼š{total:,} ç­†ï¼›ç¬¬ {page} / {total_pages} é ")
    st.data_editor(df_page, column_config=col_cfg, use_container_width=True, hide_index=True, disabled=True)

with tab_nodes:
    st.subheader("sk ç¯€é»ï¼ˆä¸å¥—ç”¨æœå°‹ / ä¸åˆ†é ï¼Œå›ºå®šé¡¯ç¤º id, sk1, sk2, sk3ï¼‰")
    if missing_sk:
        st.error("æ­¤ CSV ä¸åŒ…å« sk1ã€sk2ã€sk3 ä¸‰æ¬„ï¼Œç„¡æ³•é¡¯ç¤º sk ç¯€é»è¡¨ã€‚è«‹è£œé½Šå¾Œå†è©¦ã€‚")
    else:
        # è®€å–å®Œæ•´è³‡æ–™ï¼ˆå¿½ç•¥ WHERE èˆ‡åˆ†é ï¼‰ï¼Œåƒ…å–éœ€è¦çš„æ¬„ä½
        base_cols = ["sk1", "sk2", "sk3"]
        cols_exist = [c for c in ["id"] + base_cols if c in cols]
        sel_cols_nodes = ", ".join([f'"{c}"' for c in cols_exist])
        q_nodes = f"SELECT {sel_cols_nodes} FROM {scan}"
        df_nodes_full = con.execute(q_nodes).fetchdf().fillna("ï¼ˆç¼ºå€¼ï¼‰")
        if "id" not in df_nodes_full.columns:
            df_nodes_full.insert(0, "id", "")
        df_nodes_full = df_nodes_full[["id", "sk1", "sk2", "sk3"]]

        view = st.radio("æª¢è¦–æ–¹å¼", ["åŸå§‹åˆ—ï¼ˆid, sk1, sk2, sk3ï¼‰", "å”¯ä¸€çµ„åˆ + è¨ˆæ•¸ï¼ˆsk1, sk2, sk3ï¼‰"], horizontal=True)
        if view.startswith("å”¯ä¸€"):
            df_nodes = (
                df_nodes_full[["sk1","sk2","sk3"]]
                .value_counts(["sk1","sk2","sk3"])  # pandas >= 1.4
                .rename("count").reset_index()
                .sort_values("count", ascending=False)
            )
        else:
            df_nodes = df_nodes_full

        st.data_editor(df_nodes, use_container_width=True, hide_index=True, disabled=True)

        cna1, cna2 = st.columns(2)
        with cna1:
            st.download_button("ä¸‹è¼‰ sk ç¯€é»ï¼ˆç•¶å‰æª¢è¦–ï¼‰", data=df_nodes.to_csv(index=False).encode("utf-8-sig"), file_name="sk_nodes.csv", mime="text/csv")
        with cna2:
            st.download_button("ä¸‹è¼‰ sk åŸå§‹ï¼ˆid, sk1, sk2, sk3ï¼‰", data=df_nodes_full.to_csv(index=False).encode("utf-8-sig"), file_name="sk_raw.csv", mime="text/csv")

with tab_sankey:
    st.subheader("Sankeyï¼ˆå›ºå®šä½¿ç”¨ sk1 â†’ sk2 â†’ sk3ï¼›ä¸å¥—ç”¨æœå°‹ã€ä¸åˆ†é ï¼‰")
    if missing_sk:
        st.error("æ­¤ CSV ä¸åŒ…å« sk1ã€sk2ã€sk3 ä¸‰æ¬„ï¼Œç„¡æ³•ç¹ªè£½ Sankeyã€‚è«‹è£œé½Šå¾Œå†è©¦ã€‚")
    else:
        try:
            import plotly.graph_objects as go
        except ModuleNotFoundError:
            st.error("""æ‰¾ä¸åˆ° Plotlyã€‚è«‹å…ˆå®‰è£ï¼š`pip install plotly` æˆ– `pip3 install plotly`ã€‚è‹¥ç”¨ Condaï¼š`conda install -c plotly plotly`ã€‚
ï¼ˆTabs å·²é¡¯ç¤ºï¼›å®‰è£å¾Œé‡å•Ÿå³å¯é¡¯ç¤º Sankeyï¼‰""")
        else:
            # å–èˆ‡ã€Œskç¯€é»ã€ä¸€è‡´çš„è³‡æ–™ä¾†æºï¼ˆå®Œæ•´ã€ä¸åˆ†é ã€ä¸æœå°‹ï¼‰
            q_nodes_for_sankey = f"SELECT \"sk1\", \"sk2\", \"sk3\" FROM {scan}"
            df_nodes_for_sankey = con.execute(q_nodes_for_sankey).fetchdf().fillna("ï¼ˆç¼ºå€¼ï¼‰")
            work = df_nodes_for_sankey[["sk1","sk2","sk3"]].copy()

            # å»ºç«‹ linksï¼ˆsk1â†’sk2ã€sk2â†’sk3ï¼‰
            links = []
            for a, b in [("sk1","sk2"),("sk2","sk3")]:
                g = work.groupby([a,b], dropna=False, as_index=False).size()
                g.rename(columns={"size":"value", a:"src", b:"dst"}, inplace=True)
                links.append(g)
            links_df = pd.concat(links, ignore_index=True)

            vmax = int(max(1, int(links_df["value"].max())))
            min_val = st.slider("éæ¿¾ï¼šæœ€å°æ¬Šé‡", 1, vmax, value=1)
            links_df = links_df[links_df["value"] >= min_val]

            if links_df.empty:
                st.info("éæ¿¾å¾Œæ²’æœ‰é€£ç·šå¯é¡¯ç¤ºï¼Œè«‹æ”¾å¯¬é–€æª»æˆ–æ›´æ›è³‡æ–™æ¢ä»¶ã€‚")
            else:
                labels = pd.unique(pd.concat([work["sk1"], work["sk2"], work["sk3"]], ignore_index=True)).tolist()
                idx = {lab: i for i, lab in enumerate(labels)}
                links_df = links_df.assign(
                    source_id=links_df["src"].map(idx),
                    target_id=links_df["dst"].map(idx),
                )

                fig = go.Figure(data=[go.Sankey(
                    node=dict(
                        label=labels,
                        pad=20,
                        thickness=18,
                        color="#FFFFFF",
                        line=dict(color="rgba(0,0,0,0)", width=0),
                    ),
                    link=dict(
                        source=links_df["source_id"],
                        target=links_df["target_id"],
                        value=links_df["value"],
                        color="rgba(90,123,216,0.18)",
                    ),
                    textfont=dict(color="#0B1220", size=16, family="Microsoft JhengHei, Heiti TC, sans-serif"),
                )])
                fig.update_layout(
                    title_text="Sankeyï¼šsk1 â†’ sk2 â†’ sk3",
                    font_size=16,
                    font=dict(family="Microsoft JhengHei, Heiti TC, sans-serif", color="#0B1220"),
                    paper_bgcolor="#FFFFFF",
                    plot_bgcolor="#FFFFFF",
                    margin=dict(l=10, r=10, t=40, b=10),
                    hoverlabel=dict(bgcolor="#FFFFFF", font_size=14, font_family="Microsoft JhengHei, Heiti TC, sans-serif"),
                )
                st.plotly_chart(fig, use_container_width=True)

# â€”â€” å´æ¬„ï¼šä¸‹è¼‰å€ â€”â€”
with st.sidebar:
    st.subheader("ä¸‹è¼‰")
    st.caption("ä¸‹è¼‰ç•¶å‰é é¢æˆ–å®Œæ•´ç¯©é¸çµæœ")
    st.download_button("ä¸‹è¼‰ç•¶é ", df_page.to_csv(index=False).encode("utf-8"), "page.csv", "text/csv")
    if st.button("ç”¢ç”Ÿå®Œæ•´ CSV"):
        out = "/tmp/filtered.csv"
        con.execute(
            f"COPY (SELECT {select_cols_sql} FROM {scan} WHERE {where}) TO '{out}' (HEADER, DELIMITER ',')",
            params,
        )
        with open(out, "rb") as f:
            st.download_button("ä¸‹è¼‰å®Œæ•´çµæœ", f, "filtered.csv", "text/csv")
